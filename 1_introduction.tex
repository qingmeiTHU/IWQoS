\section{Introduction}
% its becoming popular
Recent years have seen the coming of age of personalized live
streaming. With more personal devices equipped with high-definition
cameras, we observe a rapid proliferation of apps that allow users
to stream videos from their smartphones or tablets to anyone who
tunes in. Such personalized live streaming has found its worldwide
popularity as a way of engaging with more followers (e.g., Twitter 
Meerkat~\cite{twitter}, Panda Tv~\cite{panda}),
sharing richer experience (e.g., Facebook Live~\cite{facebook}, Periscope~\cite{periscope}),
and broadcasting online gaming and sports events (e.g., Twitch~\cite{twitch}, Douyu~\cite{douyu}).


% what's new
While recent work on personalized live streaming has insofar 
focused on analyzing its traffic pattern (e.g.,~\cite{zhang2015crowdsourced,tang2016meerkat})
and video distribution architecture (e.g.,~\cite{siekkinen2016first,wang2016anatomy}),
there has not been enough effort to characterize the 
quality issues of {\em broadcaster-uploaded videos} in the wild in popular platforms.
Yet, we argue that {\em understanding and improving the broadcaster-side uploading
video quality is crucial to the Quality of Experience (QoE) in the whole personalized live streaming service.}
Broadcaster-side quality issues
have a direct impact on {\em all} viewers.
Any delay or failure caused by the broadcaster could inflate the
streaming delay of all viewers. Moreover, the upstream video quality
sets a ``cap'' on the QoE of all
viewers (even if they have high-speed downlink connections).
As a result, for instance, the broadcasters typically only upload 
videos in the highest constant bitrate.

Despite its significance, the broadcaster-side video uploading has its own unique characteristics that are not well-studied. Compared with traditional live streaming systems for popular events
(e.g., ESPN) where broadcasters have well-provisioned
connections and streaming delay is typically at the timescales 
of tens of seconds, in personalized living streaming, (1) the broadcaster has to handle variable network conditions because the wireless signal is more dynamic than wired one (user motion, signal fading and interference, etc.), (2) the end-to-end streaming delay must be below several seconds 
to create real-time interactivity when
the broadcaster interacts with viewers who pose questions
or send ``likes''.
The new challenge of personalized live streaming can be summarized as {\em provisioning high QoE (video quality and timeliness) in a variable network condition.}


% quality today is not good
Directly applying traditional frameworks in personalized streaming leads to the QoE of broadcaster-uploaded video far from being deal. Through our measurement on popular video streaming platforms, we observe two {\em prevalent} quality issues across many popular platforms in the wild.  (1) In particular, we observe an 
{\em amplifying effect of transient fluctuating network conditions} causing persistent video QoE degradation: e.g., a throughput degradation of less than a second on the broadcaster side can lead to several seconds of video stalls observed by the viewers. (2)  These broadcasters are unable to effectively respond to long-term throughput drops too. 
Such problem can easily cause significant quality degradation in practice, because the broadcasters (e.g., smartphones, tablets) are often subject to both transient and long-term wireless throughput fluctuations, which are caused by cellular hand-off, 
WiFi-cellular switches, device motion, and so forth.


% we figured out the root cause
The root cause of this amplifying effect lies in the fact that RTMP, 
the de-facto video broadcaster-side streaming protocol, can drop video frames too aggressively when the video buffer overflows, resulting in unnecessary drops of important video frames and consequently persistent video stalls experienced by viewers.
Moreover, straightforward strawman solutions (e.g., increasing
buffer length, alternative frame-dropping policies) fail to meet at least one of the two critical QoE requirements of personalized streaming: they either increase end-to-end delay (i.e., low timeliness), or drop more frames than needed (i.e., lowing video resolution).
For instance, simply increasing buffer size on the broadcaster
side hides transient throughput drops but may cause end-to-end
delay growing unboundedly.

For the long-term network packet drops, existing solutions mainly focus on the bitrate adaptation strategy of the viewer-side player, who typically maintains a long buffer of around 10 seconds. Mostly used in video-on-demand (VoD) cases,
these solutions fail miserably when used by a broadcaster of personalized live streaming, since the broadcaster typically has at most one second worth of video in its buffer. 

% key insight in our solution
In this paper, we present GVBR, a suite of solution that substantially improves
the broadcaster-side video quality in personalized live streaming.
Our key insight is that these broadcaster-side quality issues can be mitigated 
by a systematic co-design of key RTMP configuration (i.e., key frame
interval, buffer size), frame-level control logic (i.e., frame-dropping policy), 
and higher-level bitrate adaptation strategy, all of which take
video resolution and timeliness as objectives. 
While integrating GVBR in existing broadcaster involves changes in multiple
levels of the streamer stack, all changes are non-intrusive, either changing 
tunable parameters (e.g., key frame interval) or changing control logic that
is not hard-coded in the software (e.g., frame-dropping logic and bitrate 
adaptation strategies). 


Our preliminary evaluation shows that a better RTMP design
could significantly improve video quality compared to three popular
RTMP-based commercial  platforms as well as an open-source
RTMP platform.
Through extensive evaluation under a variety of network conditions, we find that 
GVBR can reduce the frame drops by 50\%, and cut video interruption incidents 
by 90\%, while achieving comparable bitrate.

%The proposed greedy algorithm, which is called GA, can reduce the frame dropping into an acceptable level(cut down $80\%$ of the frame dropping) and keep the original bitrate at the same time.

%develop a framework that take into account XXX, which are ignored in prior work, and give an optimal offline frame-dropping algorithm for RTMP.
%Furthermore, we present a practical online frame-dropping algorithm called \jc{give the algorithm a name} that empirically achieves close-to-optimal quality and outperforms baseline solutions across different metrics.


% two contributions
%% show its prevalence, and root cause
%% offer a framework, and potential solutions
In short, we make two contributions:
\begin{enumerate}
\item We are the first to shed light on the broadcaster-side video 
quality issues across three today's personalized streaming platforms and 
identify its root cause.
Through measurements on multiple popular live video streaming platforms, 
we identify a prevalent broadcaster-side quality issue, caused
by unnecessarily persistent video interruptions in the presence of short-term
bandwidth fluctuations
%\item We propose three principles to guide the broadcaster design, and for each item, give detailed formulation.
%\item We show both qualitatively and quantitatively that there is a significant room for improving the broadcaster-side video quality by a better design of frame drop scheme and bitrate adaptation algorithm.
\item We present a holistic suite of solutions that systematically address
the observed quality issue via better designs for encoding of frames, 
frame prioritization strategies, as well as bitrate adaptation strategy that 
operates at the level of groups of frames.

%Offer a formal model to qualitatively show the limitations of baseline solutions and empirically quantify the substantial room for improvement by an optimal offline frame-dropping algorithm, and as an early step, propose a practical algorithm that achieves close-to-optimal quality.
\end{enumerate}


\iffalse

\section{Introduction}
Recent years have witnessed a proliferation of commercial platforms for user-generated live streaming. For example, Twitch reported 241 billion minutes video streaming from individual broadcasters (about 459 thousand years).

In contrast to traditional live streaming (e.g., ESPN) and video on demand, user-generated video streaming two fundamental different requirements. First, the broadcaster side should try to improve video quality in an unstable network (or wireless) environment. For example, occasional network jitters or low throughput are possible when streaming outdoor activities~\cite{xx}. Second, timeliness is a key performance index when a broadcaster streams, because they may need to interact with the audience (e.g., answer questions from chatting system).

Through extensive experiments on existing commercial platforms, we find that none of them can satisfy the above two requirements simultaneously. We observe a "cascading effect" in video streaming. That is, an occasional network jitter (e.g., 100s of ms) can cause video playing be stalled by a much longer time (several seconds). We quantify this effect by experiments and provide a detailed analysis of this cascading effect.

Based on the analysis, we argue that existing alternatives and intuitive baseline solutions fail to achieve the two basic requirements above. HTTP-based video protocols usual divide video into chunks, which granularity cannot satisfy the timeliness requirement; RTMP-based protocols (as we experiments) cannot handle network jitters; and naÃ¯ve solution such as increasing video buffer to be resistant to dynamic network conditions would increase video buffering delay, which is not preferred.

To find a solution to handle dynamic network conditions in user-generated video live streaming, we first build a formal model to qualitatively compute the optimal solution and show the limitations of baseline solutions. The model also shows that in certain environment settings, there is no low-latency solution, which inspires us to reconsider video frame coding design to be resistant to network jitters.

In this paper, we make the following contributions
\begin{itemize}
\item Shed light on the root cause of the cross-layer "cascading effect" in many commercial platforms of user-generated live streaming.
\item Offer a formal model to qualitatively show the limitations of baseline solutions and empirically quantify the substantial room for improvement by an optimal offline frame-dropping algorithm,
\item As an early step, propose a practical algorithm that achieves close-to-optimal quality.
\end{itemize}


\fi
\iffalse

%1. video streaming requires timeliness, usually small buffer 2. however, in case of bad network performance, the video quality is not good enough 3. state of the art: windows based, small threshold 4. however, we claim that the solution can be further improved 5. producer-consumer, we leverage the dependency of frames 6. predict tcp behavior 7. prelimary experiment

User-generated live streaming (e.g., Facebook live, twitch.tv) is gaining its popularity due to its flexibility on location and instantaneity on time. According to Twitch's Retrospective Report 2015, with an average of 1.7 million broadcasters streaming every month, Twitch produced videos of total $241441823059$ minutes each year, which equals to $459366$ years. To avoid queuing latency in live video streaming systems, the streamer side usually adopts shallow buffers or queues to improve timeliness (e.g., the 0.7-second buffer in OBS). In practical scenarios where the underline network fails to provide stable bandwidth (e.g., outdoor streaming), the streamer side usually chooses to drop frames to guarantee the timeliness, sacrificing video completeness.

To improve video quality to the best effort in the scenario of network failures, simple priority drop mechanisms are proposed. These approaches divide a video into time windows where each window contains multiple frames, sort frame by decoding priority (e.g., I frame > P frame > B frame), and send frames in each window from high to low priority by best effort.

However, we argue that with the goal of increase the number of decodable video frames, the window-based approach is not the best approach. The essential reason is that it ignores the dependency between frames across windows.

While in live video streaming systems, frames are produced by a video frame generation thread and consumed by a frame sending thread, which is a typical producer-consumer model. Frames in the buffer are organized naturally in a temporal order. We propose smarter buffer management mechanisms, where we have timeliness as the first-priority goal and meanwhile increase the most number of decodable frames.

In this work, we first measure and show how existing frame drop strategy hampers video quality. Then we model and analyze the root cause of the poor video quality. Consequently, we propose two heuristics to achieve this goal. First, in producer side, we analyze the dependency between the frames in the buffer, and drop lowest-priority frames when the buffer overflows. Second, in consumer side, we analyze TCP sending behavior, and tries to predict TCP window size. If the predicted window size exceeds the frame generation rate, we do not drop frames (tolerating a short-time overflow of the threshold). Finally we use preliminary experiments to validate our design.


\fi



